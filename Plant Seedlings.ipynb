{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import requests, glob, Augmentor, os, json\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import TerminateOnNaN\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import History\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, out_file):\n",
    "    chunk_size = 1024\n",
    "    r = requests.get(url, stream=True)\n",
    "    total_size = int(r.headers['content-length'])\n",
    "    with open(out_file, 'wb') as f:\n",
    "        for data in tqdm(iterable=r.iter_content(chunk_size=chunk_size),\n",
    "                         total=total_size/chunk_size, unit='KB'):\n",
    "            f.write(data)\n",
    "    print('{} download Complete!'.format(out_file))\n",
    "\n",
    "def extract_zipfile(data_path, zip_file):\n",
    "    if not os.path.isdir(data_path):\n",
    "        with ZipFile(zip_file, 'r') as f:\n",
    "            print('Extracting all the files now ...')\n",
    "            f.extractall(data_path)\n",
    "            print('Done!')\n",
    "\n",
    "def explore_data(data_path, labels_list):\n",
    "    images_count = []\n",
    "    X = []\n",
    "    y = []\n",
    "    plt.figure(1)\n",
    "    print('No of images in:')\n",
    "    for idx, label in enumerate(labels_list):\n",
    "        label_path = os.path.join(data_path, label)\n",
    "        images_list = glob.glob(os.path.join(label_path, '*.png'))\n",
    "        num_images = len(images_list)\n",
    "        images_count.append(num_images)\n",
    "        X += images_list\n",
    "        y += [label] * num_images\n",
    "        img_path = images_list[0]\n",
    "        img = plt.imread(img_path)\n",
    "\n",
    "        plt.subplot(3,4,idx+1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(label)\n",
    "        plt.axis('off')\n",
    "        print('{} directory: {}'.format(label, num_images))\n",
    "    print()\n",
    "    plt.show()\n",
    "    return images_count, X, y\n",
    "\n",
    "def plot_histogram(y):\n",
    "    plt.hist(y, bins=86)\n",
    "    plt.xlabel('labels')\n",
    "    plt.ylabel('no of images')\n",
    "    plt.show()\n",
    "\n",
    "def augment_images(path, num, avg_img):\n",
    "    p = Augmentor.Pipeline(path, '.', save_format='png')\n",
    "    p.random_brightness(.5, .25, .75)\n",
    "    p.random_color(.5, .25, .75)\n",
    "    p.random_contrast(.5, .25, .75)\n",
    "    p.rotate(.5, 10, 10)\n",
    "    p.sample(avg_img - num)\n",
    "\n",
    "def add_dicts(d, e):\n",
    "    for key in e.keys():\n",
    "        if key not in d:\n",
    "            d[key] = e[key]\n",
    "        else:\n",
    "            d[key] += e[key]\n",
    "    return d\n",
    "\n",
    "def plot_metrics(d):\n",
    "    plots = [i for i in d.keys() if i.find('val_') == -1]\n",
    "    plt.figure(figsize=(15,25))\n",
    "    for i, p in enumerate(plots):\n",
    "        plt.subplot(len(plots), 2, i+1)\n",
    "        plt.title(p)\n",
    "        plt.plot(d[p], label=p)\n",
    "        plt.plot(d['val_'+p], label = 'val_'+p)\n",
    "        plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    rec = true_positives / (possible_positives + K.epsilon())\n",
    "    return rec\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    pre = true_positives / (predicted_positives + K.epsilon())\n",
    "    return pre\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    rec = recall(y_true, y_pred)\n",
    "    pre = precision(y_true, y_pred)\n",
    "    return 2*((pre*rec)/(pre+rec))\n",
    "\n",
    "def get_callbacks():\n",
    "    # TerminateOnNaN\n",
    "    terminate_callback = TerminateOnNaN()\n",
    "    # Tensorboard\n",
    "    tb_callback = TensorBoard('./Graph', histogram_freq=0,\n",
    "                    write_graph=True, write_images=True)\n",
    "    # Model Checkppoint\n",
    "    ckpt_callback = ModelCheckpoint('./output_data/weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                                    verbose=1, save_weights_only=False,\n",
    "                                    mode='auto', period=5)\n",
    "    # CSV Logger\n",
    "    csv_logger = CSVLogger('./training.log')\n",
    "    # History\n",
    "    hist = History()\n",
    "    # Callbacks list\n",
    "    callbacks = [terminate_callback, tb_callback, ckpt_callback,\n",
    "                 csv_logger, hist]\n",
    "    return callbacks\n",
    "\n",
    "def data_generator(X, y, batch_size):\n",
    "    idx = 0\n",
    "    encoder = LabelBinarizer()\n",
    "    y = encoder.fit_transform(y)\n",
    "    num_batches = len(X)//batch_size\n",
    "    while True:\n",
    "        start = idx * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_X = []\n",
    "        batch_y = y[start:end, :]\n",
    "        for filename in X[start:end]:\n",
    "            img = image.load_img(filename, target_size=(299, 299, 3))\n",
    "            img = image.img_to_array(img)\n",
    "            batch_X.append(img)\n",
    "        batch_X = np.array(batch_X)\n",
    "        batch_X = preprocess_input(batch_X)\n",
    "        batch_X /= 255\n",
    "        idx += 1\n",
    "        yield batch_X, batch_y\n",
    "        if idx == num_batches:\n",
    "            idx = 0\n",
    "\n",
    "def get_model(lr):\n",
    "    # Load model\n",
    "    # include_top is used to remove all the layers after block conv5\n",
    "\n",
    "    model = VGG19(include_top=False, input_shape=(299, 299, 3))\n",
    "\n",
    "    # Freeze all layers\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # re-add the removed layers\n",
    "    x = model.output\n",
    "    x = Flatten(name=\"flatten\")(x)\n",
    "    x = Dense(4096, activation=\"relu\", name=\"fc1\")(x)\n",
    "    x = Dense(4096, activation=\"relu\", name=\"fc2\")(x)\n",
    "    x = Dense(num_labels, activation=\"softmax\", name=\"predictions\")(x)\n",
    "\n",
    "    # Redefine the model\n",
    "    model = Model(inputs=model.input, outputs=x, name=\"final_model\")\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    adam = optimizers.Adam(lr=lr)\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', precision, recall,\n",
    "              f1])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://vision.eng.au.dk/?download=/data/WeedData/NonsegmentedV2.zip'\n",
    "zip_file = './data.zip'\n",
    "data_path = './data'\n",
    "\n",
    "if not os.path.isfile(zip_file):\n",
    "    download_file(url, zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_zipfile(data_path, zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = os.listdir(data_path)\n",
    "num_labels = len(labels_list)\n",
    "\n",
    "print('Labels:')\n",
    "\n",
    "for idx, label in enumerate(labels_list):\n",
    "    print('{}. {}'.format(idx+1, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_count, _, y = explore_data(data_path, labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_img = sum(images_count)//len(images_count)\n",
    "\n",
    "print(avg_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "\n",
    "if not os.path.isdir('./data_copy'):\n",
    "    _ = call(['cp', '-a', data_path, './data_copy'])\n",
    "\n",
    "data_path = './data_copy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(os.listdir('./data_copy/Black-grass')) < avg_img+1:    \n",
    "    for label in labels_list:\n",
    "        label_path = os.path.join(data_path, label)\n",
    "        images_list = os.listdir(label_path)\n",
    "        if len(images_list) < avg_img:\n",
    "            augment_images(label_path, len(images_list), avg_img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X, y = explore_data(data_path, labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split data\n",
    "\n",
    "X, y = shuffle(X, y)\n",
    "\n",
    "X, X_valid, y, y_valid = train_test_split(X, y, test_size=0.1,\n",
    "                                          random_state=0, stratify=y)\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                        random_state=0, stratify=y)\n",
    "\n",
    "print('Train:', len(X), len(y))\n",
    "print('Valid:', len(X_valid), len(y_valid))\n",
    "print('Test:', len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = './output_data'\n",
    "log_file = os.path.join(output_path, 'log.csv')\n",
    "\n",
    "full_model_path = os.path.join(output_path, 'plant_vgg19.h5')\n",
    "cross_model_path = os.path.join(output_path, 'plant_vgg19_cross.h5')\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 3\n",
    "batch_size = 1 #32\n",
    "epochs = 1 #10\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross Validation\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "\n",
    "if not os.path.isfile(cross_model_path):\n",
    "\n",
    "    # Training\n",
    "    print(\"Start cross-validation training...\")\n",
    "\n",
    "    histories = []\n",
    "\n",
    "    temp_X = np.array(X)\n",
    "    temp_y = np.array(y)\n",
    "\n",
    "    for train, val in skf.split(temp_X, temp_y):\n",
    "        train_datagen = data_generator(temp_X[train], temp_y[train], batch_size)\n",
    "        valid_datagen = data_generator(temp_X[val], temp_y[val], batch_size)\n",
    "\n",
    "        history = model.fit_generator(train_datagen, steps_per_epoch=len(train)//batch_size,\n",
    "                    validation_data=valid_datagen, epochs=epochs, \n",
    "                                     validation_steps = len(val)//batch_size)\n",
    "        histories.append(history)\n",
    "\n",
    "    model.save(cross_model_path)\n",
    "    \n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full Training\n",
    "\n",
    "batch_size = 1 #32\n",
    "\n",
    "epochs = 1 #20\n",
    "\n",
    "model = load_model(cross_model_path, custom_objects={'f1_micro':f1_micro, 'precision_micro':precision_micro, \n",
    "                                                     'recall_micro':recall_micro})\n",
    "\n",
    "if not os.path.isfile(full_model_path):\n",
    "\n",
    "    print(\"Full training...\")\n",
    "\n",
    "    train_datagen = data_generator(X, y, batch_size)\n",
    "    valid_datagen = data_generator(X_valid, y_valid, batch_size)\n",
    "\n",
    "    history = model.fit_generator(train_datagen, steps_per_epoch=len(X)//batch_size,\n",
    "                        epochs=epochs, callbacks = callbacks,\n",
    "                        validation_data=valid_datagen, validation_steps=len(X_valid)//batch_size)\n",
    "\n",
    "    histories.append(history)\n",
    "\n",
    "    print(\"Save whole model...\")\n",
    "    model.save(full_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "model = load_model(full_model_path, custom_objects={'f1_micro':f1_micro})\n",
    "\n",
    "test_datagen = data_generator(X_test, y_test, batch_size)\n",
    "\n",
    "Eval = model.evaluate_generator(test_datagen, steps = len(X_test)//batch_size, workers=12)    \n",
    "print(Eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
